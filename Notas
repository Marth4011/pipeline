curl -X POST "localhost:9200/test/_doc/1?pipeline=check_url&pretty" -H 'Content-Type: application/json' -d'
{
  "href": {
    "url": "http://www.elastic.co/"
  }
}
'

Levantar el Docker: docker-compose up


GET: Obtener datos. Ej: GET /v1/empleados/1234
PUT: Actualizar datos. Ej: PUT /v1/empleados/1234
POST: Crear un nuevo recurso. Ej: POST /v1/empleados
DELETE: Borrar el recurso. Ej: DELETE /v1/empleados/1234
¿PATCH?: Para actualizar ciertos datos


1.- POST: Crear un nuevo recurso. Ej: POST /v1/empleados
2.- Obtener datos. Ej: GET /v1/empleados/1234


https://medium.com/yom-ai/api-rest-con-node-js-y-elasticsearch-a2a2ca858cbb


Cliente java REST de alto nivel de elasticsearch RestHighLevelClient
https://www.adictosaltrabajo.com/2019/02/19/cliente-java-rest-de-alto-nivel-de-elasticsearch-resthighlevelclient/


Básico de ELK
https://bigdatadummy.com/2018/11/27/elasticsearch/

tutorial de Kibana
https://www.ionos.mx/digitalguide/online-marketing/analisis-web/tutorial-de-kibana/


input de conf
https://www.elastic.co/es/blog/how-to-ingest-data-into-elasticsearch-service

Plantilla de indexacion
https://www.elastic.co/guide/en/elasticsearch/reference/6.2/indices-templates.html

cargar un rdd a elastic con spark
https://www.youtube.com/watch?v=yWb367NzBlA

API REST con java, métodos get 
https://www.youtube.com/watch?v=fa1AO1cp9NM


outputs de elasticsearch
https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html

ingestar un csv con ingest node
https://www.elastic.co/es/blog/indexing-csv-elasticsearch-ingest-node



Crar el Docker
https://www.youtube.com/watch?v=R8BumGBfYME
Instalar todos los servicios en Ubuntu
https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elastic-stack-on-ubuntu-18-04-es


levantar los contenedores del docker
https://www.youtube.com/watch?v=6bXSfjwQVIc


http poller plugin
https://www.youtube.com/watch?v=6Kmd1bpDNXE

Vídeo del dashboard en Kiba con geolocation(Indú)
https://www.youtube.com/watch?v=1arB6i61qWs

----Flujo 1

	Enviar el Json a elasticsearch

			while read f1
		do        
		   curl -XPOST 'https://XXXXXXX.us-east-1.aws.found.io:9243/subway_info_v1/station' -H "Content-Type: application/json" -u elastic:XXXX -d "{ \"station\": \"$f1\" }"
		done < NYC_Transit_Subway_Entrance_And_Exit_Data.csv


	Dependencia de conector java a elastic search

<dependency>

    <groupId>org.elasticsearch.client</groupId>

    <artifactId>elasticsearch-rest-high-level-client</artifactId>

    <version>7.5.1</version>

</dependency>



curl --location --request GET 'https://apipp.conekta.io/orders/ord_2oDSZ7p4fq3G5gdMM' \
--header 'Accept: application/vnd.conekta-v2.0.0+json' \
--header 'Content-Type: application/json; charset=utf-8' \
--header 'Authorization: Basic a2V5X0VMemfreFTfnNSV0RybnprcHd6YVYzbTk5RHc6'


POST
curl -k -XPOST -H 'Authorization:jwt eyJ6aXAiOiJERUYiLCJlbmMiOiJBMTI4R0NNIiwiYWxnIjoiUlNBLU9BRVAifQ.YKr6c_Ozm1MKD6yq3JBelOoS5giwQKthbYlQMCE9RJ1rCRLX_rvfB_2M3ePdExjBPD5yqnKcH3C_McnqR-HlCb7_kQ_mczU4hWUWoCtisOI7VD9feh22JXi9qEXx4jo3RRERbuk100lNIcPs3FjTEJI3iZeaXORJTsnBTW_OuGsJo_g4KSxMcuyYu4pbrtKzXSYYtkzPVFhnFX0hW6BFeirEpCTjial3dSlI0ohmi83DvLR4CS9rki_dSC9lIT0jjSybD6npRA9AlDS9HFJUOxu7t0yBsHyMw97dFdkv2gkCKLIUerZd8eFBliFKjGlNmYrxltexlsDikfRFs6zErA.m-R0DxtsNzH0u3_N.NfVlEsfCoC32m0VdFFQ3Pcd_HyFQBFwkv4vC1NeMe60urCdIDtPEDLhTaq6uOCucRHVEggy4mL8FWsdXTeElObk2qTNpOk9pwzCoiJJarpZDlwc0F-WLKsLDA7LO-2OXV2U6vyrt6--oVew5JRSmLdRUoMhDzM15h6dQ_JfHl67Nm4wiouPho0ifayjF_u6lPf-FENmtu0UFCqg_1ysFAFMKbIY.nQY4MOYhGxMvfQM2CpuB1A' -H 'Content-Type: application/json' -d @salidasalesforce.json "https://daas.work.mx.ether.igrupobbva/services/automation-api/v2/ns/mbmi-dev/groups/processing/jobs"



curl --request GET "https://datos.cdmx.gob.mx/api/records/1.0/search/?dataset=prueba_fetchdata_metrobus&q=" > datos_metrobus.json



Vídeo de configuración de todo el stack
https://www.youtube.com/watch?v=4rju8N60T7Q
--------------------------
-----Pipeline
--------------------------
input {
  http_poller {
    urls => {
       urlname => "https://datos.cdmx.gob.mx/api/records/1.0/search/?dataset=prueba_fetchdata_metrobus&q="
    }
    request_timeout => 60
    schedule => { every => "20s" }
    codec => "line"
  }
}
output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "elastic_pipeline"
    user => "elastic"
    password => "Datapipeline"
  }
}




--output default
output {
        elasticsearch {
                hosts => "elasticsearch:9200"
                user => "elastic"
                password => "changeme"
        }
}




V1---------------------------
input {
  http_poller {
    urls => {
       urlname => "https://datos.cdmx.gob.mx/api/records/1.0/search/?dataset=prueba_fetchdata_metrobus&q="
    }
    request_timeout => 60
    schedule => { every => "20s" }
    codec => "line"
  }
}
output {
  elasticsearch {
    index => "elastic_pipeline"
    user => "elastic"
    password => "Datapipeline"
  }
}
-----------------------------


V2-----------------------------
input {
  http_poller {
    urls => {
       urlname => "https://datos.cdmx.gob.mx/api/records/1.0/search/?dataset=prueba_fetchdata_metrobus&q="
    }
    request_timeout => 60
    schedule => { every => "20s" }
    codec => "line"
  }
}

filter {
    json{
      source => "message"
    }
}

output {
  elasticsearch {
    index => "pipeline2"
    hosts => "elasticsearch:9200"
    user => "elastic"
    password => "changeme"
  }
}

---------------------------------------
Bien pero muchos registros y fields
V3
input {
  http_poller {
    urls => {
       urlname => "https://datos.cdmx.gob.mx/api/records/1.0/search/?dataset=prueba_fetchdata_metrobus&q="
    }
    request_timeout => 60
    schedule => { every => "20s" }
    codec => "json"
  }
}


output {
  elasticsearch {
    index => "pipeline3"
    hosts => "elasticsearch:9200"
    user => "elastic"
    password => "changeme"
  }
}
-----------------------------------------
start_position => "beginning"??

V4
input {
  http_poller {
    urls => {
       urlname => "https://datos.cdmx.gob.mx/api/records/1.0/search/?dataset=prueba_fetchdata_metrobus&q="
    }
    request_timeout => 60
    schedule => { every => "20s" }
    codec => "line"
  }
}

filter {
    json{
      source => "message"
      remove_field => ["[message]"]
    }
}

output {
  elasticsearch {
    index => "pipeline2"
    hosts => "elasticsearch:9200"
    user => "elastic"
    password => "changeme"
  }
}

-----------------------------------------
start_position => "beginning"??, dejá todo en message

V5
input {
  http_poller {
    urls => {
       urlname => "https://datos.cdmx.gob.mx/api/records/1.0/search/?dataset=prueba_fetchdata_metrobus&q="
    }
    request_timeout => 60
    schedule => { every => "20s" }
    codec => "line"
  }
}


output {
  elasticsearch {
    index => "pipeline5"
    hosts => "elasticsearch:9200"
    user => "elastic"
    password => "changeme"
    codec => "json"
  }
}

-----------------------------------------
start_position => "beginning"??
en filter, geopoint?

V6

input {
  http_poller {
    urls => {
       urlname => "https://datos.cdmx.gob.mx/api/records/1.0/search/?dataset=prueba_fetchdata_metrobus&q="
    }
    request_timeout => 60
    schedule => { every => "20s" }
    start_position => "beginning"
    codec => "line"
  }
}

filter {
    json{
      source => "message"
      remove_field => ["[message]"]
    }
}

output {
  elasticsearch {
    index => "pipeline6"
    hosts => "elasticsearch:9200"
    user => "elastic"
    password => "changeme"
    codec => "json"
  }
}

-----------------------------------------

sudo docker stack rm elk-stack

sudo service docker start

sudo docker stack deploy -c docker-stack.yml elk-stack

sudo docker node ls

sudo docker stack ps elk-stack


docker network inspect my-overlay

--Me salí de swarm: docker swarm leave --force
	Volví a entrar: docker swarm init

----a ver
Solved it :smile:
->Removed swarm completely.
->Checked, whether every node has docker_gwbridge network , while doing ‘docker network ls’
->Created docker_gwbridge network for nodes that doesn’t have one.

docker network create
–subnet 10.11.0.0/16
– gateway 10.11.0.1
–opt com.docker.network.bridge.name=docker_gwbridge
–opt com.docker.network.bridge.enable_icc=false
–opt com.docker.network.bridge.enable_ip_masquerade=true
docker_gwbridge
(Gave my network subnet and gateway address)

-> finally created docker swarm, as usual with docker swarm init --advertise … command.



---UY
sudo docker-compose up -d

posible salida host: http://elastic:xxxxxx@localhost:9200/


sh_1       | [2020-10-12T06:09:06,809][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://elastic:xxxxxx@localhost:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>"Elasticsearch Unreachable: [http://elastic:xxxxxx@localhost:9200/][Manticore::SocketException] Connection refused (Connection refused)"}


logstash_1       | [2020-10-12T17:33:07,964][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>"http://elastic:xxxxxx@elasticsearch:9200/", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::BadResponseCodeError, :error=>"Got response code '401' contacting Elasticsearch at URL 'http://elasticsearch:9200/'"}

1.-
/home/martha/Documentos/Pipeline/docker-elk/logstash/config/logstash.yml
Original: xpack.monitoring.elasticsearch.hosts: [ "http://elasticsearch:9200" ]
cambio: xpack.monitoring.elasticsearch.hosts: [ "http://localhost:9200" ]


Plan 1
agregar el pipeline y probar



Requerimientos y reglas de negocio
LISTO● Presentar un diagrama con el diseño de su solución
LISTO● Consultar periódicamente la fuente de datos
     ● Obtener la alcaldía correspondiente a cada posición
LISTO● Almacenar la información en una base de datos
	 ● Diseñar e implementar un API que permita consultar la información almacenada, con las
	     siguientes características:
	     ○ Obtener una lista de unidades disponibles
	     ○ Obtener una lista de alcaldías disponibles
	     ○ Consultar los el historial de ubicaciones/fechas de una unidad dado su ID
         ○ Obtener una lista de unidades que hayan estado dentro de una alcaldía